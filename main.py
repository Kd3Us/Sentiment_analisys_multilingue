"""
Point d'entr√©e principal de l'application Sentiment AI Platform.
Lance le serveur FastAPI avec toutes les configurations n√©cessaires.
"""

import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import time
from datetime import datetime
from loguru import logger
import sys
from pathlib import Path

# Configuration des logs
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level="INFO"
)
logger.add(
    "logs/app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG",
    rotation="100 MB",
    retention="30 days"
)

from app.config.settings import settings
from app.api.routes import router
from app.api.models import ErrorResponse


# Gestionnaire de cycle de vie de l'application
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire du cycle de vie de l'application"""
    
    # Startup
    logger.info("üöÄ D√©marrage de Sentiment AI Platform")
    
    # Cr√©er les dossiers n√©cessaires
    Path(settings.MODELS_PATH).mkdir(parents=True, exist_ok=True)
    Path(settings.DATA_PATH).mkdir(parents=True, exist_ok=True)
    Path(settings.LOGS_PATH).mkdir(parents=True, exist_ok=True)
    
    # V√©rifier les d√©pendances
    try:
        import torch
        logger.info(f"‚úì PyTorch {torch.__version__} - CUDA: {torch.cuda.is_available()}")
        
        import nltk
        logger.info("‚úì NLTK disponible")
        
        import spacy
        logger.info("‚úì spaCy disponible")
        
        from langdetect import detect
        logger.info("‚úì D√©tection de langue disponible")
        
    except ImportError as e:
        logger.error(f"‚ùå D√©pendance manquante: {e}")
        raise
    
    logger.info("‚úÖ Toutes les d√©pendances sont pr√™tes")
    
    yield
    
    # Shutdown
    logger.info("üõë Arr√™t de Sentiment AI Platform")


# Cr√©er l'application FastAPI
app = FastAPI(
    title=settings.PROJECT_NAME,
    version=settings.VERSION,
    description="""
    # Sentiment AI Platform
    
    Plateforme d'analyse de sentiment multilingue avec mod√®les personnalisables.
    
    ## Fonctionnalit√©s principales
    
    * üß† **Mod√®les from scratch** - Architectures LSTM, CNN, Transformer, Hybride
    * üåç **Multilingue** - Support automatique de multiples langues
    * üìä **5 nuances** - Classification fine du sentiment
    * üéØ **Personnalisable** - Chaque client entra√Æne son propre mod√®le
    * ‚ö° **Temps r√©el** - Suivi de l'entra√Ænement via WebSocket
    * üîÑ **API REST** - Interface simple et compl√®te
    
    ## Workflow typique
    
    1. **Upload** des donn√©es d'entra√Ænement annot√©es
    2. **Configuration** du mod√®le (architecture, hyperparam√®tres)
    3. **Entra√Ænement** avec suivi temps r√©el
    4. **Pr√©diction** sur nouveaux textes
    5. **Monitoring** des performances
    
    ## Architectures disponibles
    
    * **LSTM** - R√©seaux r√©currents bidirectionnels avec attention
    * **CNN** - Convolutions 1D avec multiple kernel sizes
    * **Transformer** - Architecture d'attention pure
    * **Hybrid** - Combinaison CNN + LSTM
    """,
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# Middleware CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √Ä configurer selon vos besoins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Middleware de compression
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Middleware de logging des requ√™tes
@app.middleware("http")
async def log_requests(request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(
        f"{request.method} {request.url.path} - "
        f"Status: {response.status_code} - "
        f"Time: {process_time:.3f}s"
    )
    
    return response

# Gestionnaire d'erreurs global
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(
            error=f"HTTP_{exc.status_code}",
            message=str(exc.detail),
            timestamp=datetime.now().isoformat(),
        ).dict()
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"Erreur non g√©r√©e: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error="INTERNAL_SERVER_ERROR", 
            message="Une erreur interne s'est produite",
            timestamp=datetime.now().isoformat(),
        ).dict()
    )

# Inclure les routes
app.include_router(router, tags=["Sentiment Analysis"])

# Routes de base
@app.get("/", tags=["Root"])
async def root():
    """Page d'accueil de l'API"""
    return {
        "message": "Bienvenue sur Sentiment AI Platform! üöÄ",
        "version": settings.VERSION,
        "docs": "/docs",
        "status": "running",
        "features": [
            "Mod√®les from scratch",
            "Support multilingue", 
            "5 niveaux de sentiment",
            "Entra√Ænement personnalis√©",
            "API REST compl√®te"
        ]
    }

@app.get("/info", tags=["Info"])
async def get_info():
    """Informations d√©taill√©es sur la plateforme"""
    import torch
    
    return {
        "platform": {
            "name": settings.PROJECT_NAME,
            "version": settings.VERSION,
            "debug": settings.DEBUG
        },
        "supported_features": {
            "architectures": settings.AVAILABLE_ARCHITECTURES,
            "languages": settings.SUPPORTED_LANGUAGES,
            "sentiment_levels": f"{settings.MIN_SENTIMENT_LEVELS}-{settings.MAX_SENTIMENT_LEVELS}",
            "file_formats": settings.ALLOWED_FILE_TYPES
        },
        "system": {
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_devices": torch.cuda.device_count() if torch.cuda.is_available() else 0
        },
        "limits": {
            "max_upload_size_mb": settings.MAX_UPLOAD_SIZE_MB,
            "max_training_time_hours": settings.MAX_TRAINING_TIME_HOURS,
            "min_training_samples": settings.MIN_TRAINING_SAMPLES
        }
    }


# Point d'entr√©e pour le d√©veloppement
if __name__ == "__main__":
    logger.info(f"üéØ Lancement du serveur sur {settings.HOST}:{settings.PORT}")
    
    uvicorn.run(
        "main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        workers=1,  # Important pour le stockage en m√©moire
        log_level="info"
    )